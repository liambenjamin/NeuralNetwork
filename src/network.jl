#Author: Vivak Patel & Liam Johnston
#Date: April 25, 2019
#Description: Implements and verifies general feed forward network structure

module Network

using LinearAlgebra, SparseArrays, Main.Neurons

export paramGrad, inputGrad, update!


mutable struct network
    neurons :: Vector{neuron}            #List of neurons
    graph :: SparseMatrixCSC             #Specification of feed forward relationships
    neur2X :: Dict{Int64,Vector{Int64}}  #Given neural index, specifies indices of X generated by neuron
    X2neur :: Vector{Int64}              #Given X index, specifies neuron that generates it
    n2neur :: Dict{Int64,Vector{Int64}}  #Given neuron, specifies indices of neurons that feed into it
    n2indX :: Dict{Int64,Vector{Int64}}  #Given neuron, specifies indices of X that feed into it
    features :: Int64                    #Dimension of feature vector
    results :: Int64                     #Dimension of results vector
    hid_dim :: Int64                     #Dimension of hidden layer
    fc_dim :: Int64                      #Dimension of fully connnect layer
    seq_length :: Int64                  #Length of sequence (number of recursive operations in RNN or time horizon)
end

"""
Constructor for network with empty graph
"""
function network(neurons :: Vector{neuron}, feature_size, results_size, hid_dim, fc_dim, seq_length)
    L = length(neurons)

    #Construct neur2X

    #(1) Are neurons sorted by index?
    [neuron.ide for neuron in neurons] == 1:L || @error "Neurons are not sorted by index."

    #(2) Start with feature vector as index 0
    neur2X = Dict{Int64,Vector{Int64}}(0 => collect(1:feature_size))
    strt_indx = feature_size

    #(3) Parse through sorted neurons
    for neuron in neurons
        ide = neuron.ide
        out = neuron.out
        push!(neur2X, ide => collect(strt_indx+1: strt_indx+out))
        strt_indx += out
    end

    #Construct X2neur
    X2neur = vcat(map(λ -> λ*ones( Int64, length(neur2X[λ]) ), 0:L  )  ...)

    #Construct Empty Graph (unconnected)
    mX = length(X2neur)
    graph = spzeros(mX,L+1)

    #Construct Empty n2neur and n2indX
    n2neur = Dict{Int64,Vector{Int64}}(i => Int64[] for i = 1:L)
    n2indX = Dict{Int64,Vector{Int64}}(i => Int64[] for i = 1:L)

    return network(neurons, graph, neur2X, X2neur, n2neur, n2indX, feature_size, results_size, hid_dim, fc_dim, seq_length)
end
function network(neurons :: Vector{neuron}, feature_size, results_size)
    L = length(neurons)

    #Construct neur2X

    #(1) Are neurons sorted by index?
    [neuron.ide for neuron in neurons] == 1:L || @error "Neurons are not sorted by index."

    #(2) Start with feature vector as index 0
    neur2X = Dict{Int64,Vector{Int64}}(0 => collect(1:feature_size))
    strt_indx = feature_size

    #(3) Parse through sorted neurons
    for neuron in neurons
        ide = neuron.ide
        out = neuron.out
        push!(neur2X, ide => collect(strt_indx+1: strt_indx+out))
        strt_indx += out
    end

    #Construct X2neur
    X2neur = vcat(map(λ -> λ*ones( Int64, length(neur2X[λ]) ), 0:L  )  ...)

    #Construct Empty Graph (unconnected)
    mX = length(X2neur)
    graph = spzeros(mX,L+1)

    #Construct Empty n2neur and n2indX
    n2neur = Dict{Int64,Vector{Int64}}(i => Int64[] for i = 1:L)
    n2indX = Dict{Int64,Vector{Int64}}(i => Int64[] for i = 1:L)

    return network(neurons, graph, neur2X, X2neur, n2neur, n2indX, feature_size, results_size, 0, 0, 0)
end

"""
Verifies graph structure is feed forward.
"""
function verify(ntwk :: network)
    L = length(ntwk.neurons)
    num_out = ntwk.results
    failed = falses(L)
    for i = 1:L
        ind_min = minimum(ntwk.neur2X[i]) #Get smallest output index for neuron i
        sum(ntwk.graph[ind_min:end,i]) > 0.0 && begin
            @info "Neuron $i is cylical"
            failed[i] = true
        end #None of the elements of graph with index at least smallest output index can feed back into i (upper triangular)
    end
    sum(ntwk.graph[:,L+1]) != num_out && @error "Number of outputs from graph and number of outputs supplied disagree."
    return failed
end

"""
Adds and verifies given graph structure to given network.
"""
function graph!(ntwk :: network, graph :: SparseMatrixCSC)
    num_out = ntwk.results
    L = length(ntwk.neurons)
    nX = length(ntwk.X2neur)

    #Verify graph dimensions
    size(graph) != (nX,L+1) && @error "Graph dimension does not correspond to neurons."
    ntwk.graph = graph

    #Verify outputs of graph and outputs intended align
    failed = verify(ntwk)

    sum(failed) > 0 && @error "Graph induces cycles."

    #Construct n2indX
    ntwk.n2indX = Dict{Int64,Vector{Int64}}(i => findnz(ntwk.graph[:,i])[1] for i = 1:L)

    #Construct n2neur
    ntwk.n2neur = Dict{Int64,Vector{Int64}}(i => unique(ntwk.X2neur[ntwk.n2indX[i]]) for i = 1:L)

    return true
end

"""
Evaluates network for given feature
"""
function evaluate(ntwk :: network, feat)
    #Verify feature dimension
    length(feat) != ntwk.features && @error "Feature vector dimension does not correspond to network structure."

    #Verify graph structure is initialized
    sum(ntwk.graph) == 0 && @error "Graph is not initialized."

    #Initialize forward system vector
    X = zeros(length(ntwk.X2neur))
    X[1:ntwk.features] = feat

    #Evaluates each neuron and assigns output to X
    L = length(ntwk.neurons)
    for i = 1:L
        mdl, β, met = ntwk.neurons[i].mod, ntwk.neurons[i].β, ntwk.neurons[i].met
        output = eval(mdl).act(X[ntwk.n2indX[i]], β; met = met) # X[ntwk.n2indX[i]] recover components of X feeding into neuron
        X[ntwk.neur2X[i]] = typeof(output) <: Array ? output : [output] #Indices of X generated by neuron
    end

    return X
end


"""
Computes the adjoint variables
"""
function adjoint(ntwk :: network, X :: Vector{Float64}, df :: Function)
    L = length(ntwk.neurons)
    mX = length(X)

    #Construct adjoint vector
    λ = zeros(Float64,length(X))

    #Map derivatives of f, note derivative w.r.t to X_0 is negative of first 1:ntwk.features
    λ = -df(X)

    #Reverse direction
    for i = L:-1:1
        #get ∂₁ H_i(X,β)
        mdl, β, met, indX = ntwk.neurons[i].mod, ntwk.neurons[i].β, ntwk.neurons[i].met, ntwk.n2indX[i]
        output = eval(mdl).dX(X[indX], β; met = met)

        #get λᵢ, map derivative components to components of λ
        λᵢ = λ[ntwk.neur2X[i]]
        λ[indX] += ntwk.neurons[i].out == 1 ? λᵢ[1]*output : transpose(output)*λᵢ
    end

    return λ
end

function adjoint(ntwk :: network, X :: Vector{Float64}, label :: Vector{Float64}, loss :: Module)
    L = length(ntwk.neurons)
    mX = length(X)

    #Construct adjoint vector
    λ = zeros(Float64,length(X))

    #Map derivatives of f, note derivative w.r.t to X_0 is negative of first 1:ntwk.features
    λ = -loss.gradient(label, X, ntwk.results)

    #Reverse direction
    for i = L:-1:1
        #get ∂₁ H_i(X,β)
        mdl, β, met, indX = ntwk.neurons[i].mod, ntwk.neurons[i].β, ntwk.neurons[i].met, ntwk.n2indX[i]
        output = eval(mdl).dX(X[indX], β; met = met)

        #get λᵢ, map derivative components to components of λ
        λᵢ = λ[ntwk.neur2X[i]]
        λ[indX] += ntwk.neurons[i].out == 1 ? λᵢ[1]*output : transpose(output)*λᵢ
    end

    return λ
end

"""
Computes the forward coadjoint variables
"""
function coadjoint_forward(ntwk :: network, X :: Vector{Float64}, λ :: Vector{Float64}, penalty :: Module)
    L = length(ntwk.neurons)
    mX = length(X)

    #Construct coadjoint forward vector
    if penalty == Main.penalty.test_g
        γ = -penalty.gradient(λ, ntwk.features, ntwk.hid_dim, ntwk.results)
    else
        γ = -penalty.gradient(λ, ntwk.features, ntwk.hid_dim, ntwk.seq_length)
    end

    #Forward Direction
    for i = 1:1:L
        #Get model H_i(X,beta)
        mdl, β, met, indX = ntwk.neurons[i].mod, ntwk.neurons[i].β, ntwk.neurons[i].met, ntwk.n2indX[i]

        #Get G_i gamma[i-1]
        γᵢ = γ[indX]

        #Compute dh/dz
        dhdz = eval(mdl).dX( X[indX], β; met = met)

        #Total
        output = ntwk.neurons[i].out == 1 ? γᵢ'*dhdz : dhdz*γᵢ
        ## if out==1, then dhdz is a vector and this must be accounted for.

        #Update gamma
        γ[ntwk.neur2X[i]] += typeof(output) <: Array ? output : [output]
    end

    return γ
end
function coadjoint_forward(ntwk :: network, X :: Vector{Float64}, λ :: Vector{Float64}, penalty :: Module, ntwk_type :: String)
    L = length(ntwk.neurons)
    mX = length(X)

    #Construct coadjoint forward vector
    if penalty == Main.penalty.test_g
        γ = -penalty.gradient(λ, ntwk.features, ntwk.hid_dim, ntwk.results)
    else
        γ = -penalty.gradient(λ, ntwk.features, ntwk.hid_dim, ntwk.seq_length, ntwk_type)
    end

    #Forward Direction
    for i = 1:1:L
        #Get model H_i(X,beta)
        mdl, β, met, indX = ntwk.neurons[i].mod, ntwk.neurons[i].β, ntwk.neurons[i].met, ntwk.n2indX[i]

        #Get G_i gamma[i-1]
        γᵢ = γ[indX]

        #Compute dh/dz
        dhdz = eval(mdl).dX( X[indX], β; met = met)

        #Total
        output = ntwk.neurons[i].out == 1 ? γᵢ'*dhdz : dhdz*γᵢ
        ## if out==1, then dhdz is a vector and this must be accounted for.

        #Update gamma
        γ[ntwk.neur2X[i]] += typeof(output) <: Array ? output : [output]
    end

    return γ
end

function coadjoint_forward(ntwk :: network, X :: Vector{Float64}, λ :: Vector{Float64}, dg :: Function)
    L = length(ntwk.neurons)
    mX = length(X)

    #Construct coadjoint forward vector
    γ = -dg(λ)

    #Forward Direction
    for i = 1:1:L
        #Get model H_i(X,beta)
        mdl, β, met, indX = ntwk.neurons[i].mod, ntwk.neurons[i].β, ntwk.neurons[i].met, ntwk.n2indX[i]

        #Get G_i gamma[i-1]
        γᵢ = γ[indX]

        #Compute dh/dz
        dhdz = eval(mdl).dX( X[indX], β; met = met)

        #Total
        output = ntwk.neurons[i].out == 1 ? γᵢ'*dhdz : dhdz*γᵢ
        ## if out==1, then dhdz is a vector and this must be accounted for.

        #Update gamma
        γ[ntwk.neur2X[i]] += typeof(output) <: Array ? output : [output]
    end

    return γ
end
"""
Computes the backward coadjoint variables
"""
function coadjoint_backward(ntwk :: network, X :: Vector{Float64}, λ :: Vector{Float64}, γ :: Vector{Float64}, df :: Function, d2f :: Function)
    L = length(ntwk.neurons)
    mX = length(X)

    #Compute first iteration of alpha
    α = - df(X) - d2f(X)*γ

    for i = L:-1:1
        #Compute model and derivatives
        mdl, β, met, indX = ntwk.neurons[i].mod, ntwk.neurons[i].β, ntwk.neurons[i].met, ntwk.n2indX[i]
        derX = eval(mdl).dX(X[indX], β; met = met)
        derXX = eval(mdl).dXX(X[indX], β; met = met)

        #Compute derivative-vector products
        αᵢ = α[ntwk.neur2X[i]]
        γᵢ = γ[indX]
        λᵢ = λ[ntwk.neur2X[i]]
        t = ntwk.neurons[i].out
        if t == 1
            α[indX] += αᵢ[1]*derX + λᵢ[1]*(derXX*γᵢ)
        else
            α[indX] += transpose(derX)*αᵢ + sum(map(j -> λᵢ[j]*( derXX[j]*γᵢ), 1:t))
        end
    end

    return α
end
function coadjoint_backward(ntwk :: network, X :: Vector{Float64}, label :: Vector{Float64}, λ :: Vector{Float64}, γ :: Vector{Float64}, loss :: Module)
    L = length(ntwk.neurons)
    mX = length(X)

    #Compute first iteration of alpha
    α = -loss.gradient(label,X,ntwk.results) - loss.hessian(label,X,ntwk.results) * γ

    for i = L:-1:1
        #Compute model and derivatives
        mdl, β, met, indX = ntwk.neurons[i].mod, ntwk.neurons[i].β, ntwk.neurons[i].met, ntwk.n2indX[i]
        derX = eval(mdl).dX(X[indX], β; met = met)
        derXX = eval(mdl).dXX(X[indX], β; met = met)

        #Compute derivative-vector products
        αᵢ = α[ntwk.neur2X[i]]
        γᵢ = γ[indX]
        λᵢ = λ[ntwk.neur2X[i]]
        t = ntwk.neurons[i].out
        if t == 1
            α[indX] += αᵢ[1]*derX + λᵢ[1]*(derXX*γᵢ)
        else
            α[indX] += transpose(derX)*αᵢ + sum(map(j -> λᵢ[j]*( derXX[j]*γᵢ), 1:t))
        end
    end

    return α
end

"""
Computes gradient with respect to the parameters using adjoint or coadjoint depending on number of arguments.
"""
function paramGrad(df :: Function, ntwk :: network, feat :: Vector{Float64})
    X = evaluate(ntwk, feat)
    λ = adjoint(ntwk, X, df)
    return paramGrad(ntwk, X, λ)
end
function paramGrad(loss :: Module, ntwk :: network, feat :: Vector{Float64}, label :: Vector{Float64})
    X = evaluate(ntwk, feat)
    λ = adjoint(ntwk, X, label, loss)
    return paramGrad(ntwk, X, λ)
end
function paramGrad(loss :: Module, penalty :: Module, ntwk :: network, feat :: Vector{Float64}, label :: Vector{Float64},ntwk_type :: String)
    X = evaluate(ntwk, feat)
    λ = adjoint(ntwk, X, label, loss)
    γ = coadjoint_forward(ntwk, X, λ, penalty, ntwk_type)
    α = coadjoint_backward(ntwk, X, label, λ, γ, loss)
    return paramGrad(ntwk, X, λ, γ, α)
end
function paramGrad(loss :: Module, g :: Function, ntwk :: network, feat :: Vector{Float64}, label :: Vector{Float64}, ntwk_type :: String)
    X = evaluate(ntwk, feat)
    λ = adjoint(ntwk, X, label, loss)
    γ = coadjoint_forward(ntwk, X, λ, g, ntwk_type)
    α = coadjoint_backward(ntwk, X, label, λ, γ, loss)
    return paramGrad(ntwk, X, λ, γ, α)
end
function paramGrad(df :: Function, d2f :: Function, dg :: Function, ntwk :: network, feat :: Vector{Float64})
    X = evaluate(ntwk, feat)
    λ = adjoint(ntwk, X, df)
    γ = coadjoint_forward(ntwk, X, λ, dg)
    α = coadjoint_backward(ntwk, X, λ, γ, df, d2f)
    return paramGrad(ntwk, X, λ, γ, α)
end
function paramGrad(ntwk :: network, X :: Vector{Float64}, λ :: Vector{Float64})
    L = length(ntwk.neurons)
    function getGrad(k)
        outIndx = ntwk.neur2X[k]
        inIndx = ntwk.n2indX[k]
        mdl, β, met = ntwk.neurons[k].mod, ntwk.neurons[k].β, ntwk.neurons[k].met
        output = eval(mdl).dP(X[inIndx], β; met = met)
        λₖ = λ[outIndx]
        if ntwk.neurons[k].mod == :softmax
            return [-transpose(output)*λₖ]
        else
            return ntwk.neurons[k].out == 1 ? -λₖ[1]*output : -transpose(output)*λₖ
        end
    end
    return Dict{Int64,Vector{Float64}}(map(k -> (k => getGrad(k)), 1:L))
end
function paramGrad(ntwk :: network, X :: Vector{Float64}, λ :: Vector{Float64}, γ :: Vector{Float64}, α :: Vector{Float64})
    L = length(ntwk.neurons)
    function getGrad(k)
        if ntwk.neurons[k].mod == :softmax
            return [0.0]
        else
            outIndx = ntwk.neur2X[k]
            inIndx = ntwk.n2indX[k]
            mdl, β, met = ntwk.neurons[k].mod, ntwk.neurons[k].β, ntwk.neurons[k].met
            derPar = eval(mdl).dP(X[inIndx], β; met = met)
            αₖ = α[outIndx]
            t = ntwk.neurons[k].out
            term1 = t == 1 ? -αₖ[1]*derPar : -transpose(derPar)*αₖ

            derMix = eval(mdl).dXP(X[inIndx],β; met = met)
            λₖ = λ[outIndx]
            γₖ = γ[inIndx]
            term2 = t == 1 ? -λₖ[1]*(derMix'*γₖ) : -sum(map(j -> λₖ[j]*( derMix[j]'*γₖ) ,1:t))

            return term1 + term2
        end
    end
    return Dict{Int64,Vector{Float64}}(map(k -> (k => getGrad(k)), 1:L))
end

"""
Computes gradient with respect to the input using adjoint or coadjoint depending on number of arguments.
"""
function inputGrad(df :: Function, ntwk :: network, feat :: Vector{Float64})
    X = evaluate(ntwk, feat)
    λ = adjoint(ntwk, X, df)
    return inputGrad(ntwk, X, λ)
end
function inputGrad(loss :: Module, ntwk :: network, feat :: Vector{Float64}, label :: Vector{Float64})
    X = evaluate(ntwk, feat)
    λ = adjoint(ntwk, X, label, loss)
    return inputGrad(ntwk, X, λ)
end
function inputGrad(loss :: Module, penalty :: Module, ntwk :: network, feat :: Vector{Float64}, label :: Vector{Float64}, ntwk_type :: String)
    X = evaluate(ntwk, feat)
    λ = adjoint(ntwk, X, label, loss)
    γ = coadjoint_forward(ntwk, X, λ, penalty, ntwk_type)
    α = coadjoint_backward(ntwk, X, label, λ, γ, loss)
    return inputGrad(ntwk, X, λ, γ, α)
end
function inputGrad(ntwk :: network, X :: Vector{Float64}, λ :: Vector{Float64})
    L = length(ntwk.neurons)
    function getGrad(k)
        outIndx = ntwk.neur2X[k]
        inIndx = ntwk.n2indX[k]
        mdl, β, met = ntwk.neurons[k].mod, ntwk.neurons[k].β, ntwk.neurons[k].met
        # derivative w.r.t. input
        output = eval(mdl).dX(X[inIndx], β; met = met)
        λₖ = λ[outIndx]
        if ntwk.neurons[k].mod == :softmax
            return [-transpose(output)*λₖ]
        else
            return ntwk.neurons[k].out == 1 ? -λₖ[1]*output : -transpose(output)*λₖ
        end
    end
    return Dict{Int64,Vector{Float64}}(map(k -> (k => getGrad(k)), 1:L))
end
function inputGrad(ntwk :: network, X :: Vector{Float64}, λ :: Vector{Float64}, γ :: Vector{Float64}, α :: Vector{Float64})
    L = length(ntwk.neurons)
    function getGrad(k)
        if ntwk.neurons[k].mod == :softmax
            return [0.0]
        else
            outIndx = ntwk.neur2X[k]
            inIndx = ntwk.n2indX[k]
            mdl, β, met = ntwk.neurons[k].mod, ntwk.neurons[k].β, ntwk.neurons[k].met
            # derivative w.r.t. input
            derPar = eval(mdl).dX(X[inIndx], β; met = met)
            αₖ = α[outIndx]
            t = ntwk.neurons[k].out
            term1 = t == 1 ? -αₖ[1]*derPar : -transpose(derPar)*αₖ

            derMix = eval(mdl).dXX(X[inIndx],β; met = met)
            λₖ = λ[outIndx]
            γₖ = γ[inIndx]
            term2 = t == 1 ? -λₖ[1]*(derMix'*γₖ) : -sum(map(j -> λₖ[j]*( derMix[j]'*γₖ) ,1:t))

            return term1 + term2
        end
    end
    return Dict{Int64,Vector{Float64}}(map(k -> (k => getGrad(k)), 1:L))
end


"""
Update network parameters by parameter gradient with given stepsize
"""
function update!(df :: Function, ntwk :: network, feat :: Vector{Float64}, step :: Float64)
    L = length(ntwk.neurons)
    grads = paramGrad(df, ntwk, feat)
    for i = 1:L
        ntwk.neurons[i].β -= step * grads[i]
    end
    nothing
end
function update!(loss :: Module, ntwk :: network, feat :: Vector{Float64}, label :: Vector{Float64}, step :: Float64)
    L = length(ntwk.neurons)
    grads = paramGrad(loss, ntwk, feat, label)
    for i = 1:L
        ntwk.neurons[i].β -= step * grads[i]
    end
    nothing
end
function update!(df :: Function, d2f :: Function, dg :: Function, ntwk :: network, feat :: Vector{Float64}, step :: Float64)
    L = length(ntwk.neurons)
    grads = paramGrad(df, d2f, dg, ntwk, feat)
    for i = 1:L
        ntwk.neurons[i].β -= step * grads[i]
    end
    nothing
end
function update!(loss :: Module, penalty :: Module, ntwk :: network, feat :: Vector{Float64}, label :: Vector{Float64}, step :: Float64)
    L = length(ntwk.neurons)
    grads = paramGrad(loss, penalty, ntwk, feat, label)
    for i = 1:L
        ntwk.neurons[i].β -= step * grads[i]
    end
    nothing
end
function update!(loss :: Module, penalty :: Function, ntwk :: network, feat :: Vector{Float64}, label :: Vector{Float64}, step :: Float64)
    L = length(ntwk.neurons)
    grads = paramGrad(loss, penalty, ntwk, feat, label)
    for i = 1:L
        ntwk.neurons[i].β -= step * grads[i]
    end
    nothing
end
function update!(ntwk :: network, grads :: Dict, step :: Float64)
    L = length(ntwk.neurons)
    for i = 1:L
        ntwk.neurons[i].β -= step * grads[i]
    end
    nothing
end


end #End Module
